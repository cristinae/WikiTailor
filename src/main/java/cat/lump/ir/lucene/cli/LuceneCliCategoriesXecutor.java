package cat.lump.ir.lucene.cli;

import java.io.File;
import java.net.URISyntaxException;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.OptionBuilder;
import org.apache.commons.cli.OptionGroup;

import cat.lump.aq.basics.log.LumpLogger;
import cat.lump.aq.textextraction.wikipedia.cli.WikipediaCliMinimum;
import cat.lump.aq.wikilink.jwpl.WikipediaJwpl;
import de.tudarmstadt.ukp.wikipedia.api.exception.WikiApiException;

/**
 * CLI to access the Xecutor pipeline for the WikiTailor IR-based
 * in-domain comparable corpora extraction.
 *   
 * @since Feb 7, 2016
 * @author cristina
 *
 */

public class LuceneCliCategoriesXecutor extends WikipediaCliMinimum{

	
	/**Identifier of the category */
	private String sCategory;
	private int iCategory;
		
	/**Path to the original WP articles */
	private String inputDir;
		
	/**Path to the index generated by Lucene */
	private String indexDir;

	/**Path to the output directory */
	private String outputDir;

	/**File with the domain vocabulary */
	private String dictFile;

	/**Number of step at which the process begins */
	private int firstStep;
	
	/**Number of step at which the process ends */
	private int endStep;

	/**Number of terms to be considered */
	private int top;

	
	public LuceneCliCategoriesXecutor() {
		super();
		LABEL = "Xecutor";
		logger = new LumpLogger(LABEL);
		Class<LuceneCliCategoriesXecutor> c = LuceneCliCategoriesXecutor.class;
		try {
			File exe = new File(c.getProtectionDomain().getCodeSource().getLocation().toURI().getPath());
			command = "java -cp " + exe.getName() + " cat.lump.ir.lucene.Xecutor";
			footer = "\nEx: "+ command +" -l en -y 2015 -i wikiTailor.ini " +
					 "-n 49024 -d WTlucene/rawFiles -o WTlucene/extraction \n";
		} catch (URISyntaxException e) {
			e.printStackTrace();
		}

	}
	
	/* (non-Javadoc)
	 * @see cat.lump.aq.textextraction.wikipedia.cli.WikipediaCliMinimum#loadOptions()
	 */
	/**
	 * Loads additional options: category (numerical and string), percentage of
	 * words required and output file 
	 */
	@Override
	protected void loadOptions() {
		super.loadOptions();
		//MANDATORY (at WikipediaCliMinimum)
		//options.addOption("l", "language", true, 
		//			"Language of interest (e.g., en, es, ca)");
		//options.addOption("y", "year", true, 
		//			"Dump year (e.g. 2010, 2012, 2013, 2015)");
		
		//ALTERNATIVELY
		OptionGroup group = new OptionGroup();
		Option catOption = new Option("c", "category", true, 
				"Name of the category (with '_' instead of ' '; you can use -n instead)");		
		group.addOption(catOption);
		Option numOption = new Option("n", "numcategory", true,
				"Numerical identifier of the category (you can use -c instead)");
		group.addOption(numOption);
		options.addOptionGroup(group);
		
		//OPTIONAL
		options.addOption("s", "start", true,
				"Initial step for the process \n(default: 1)");
		options.addOption("e", "end", true,
				"Last step for the process \n(default: 5)");
		options.addOption("d", "inputDir", true,
				"Directory to store/read the raw Wikipedia articles");		
		options.addOption("o", "outputDir", true,
				"Save the output into this directory \n(default: current)");		
		options.addOption("t", "top", true,
				"Number of vocabulary terms within the 10% \n(default: 100, all: -1)");		
		
		OptionBuilder.withLongOpt("indexDir");
		OptionBuilder.withDescription("Directory with the input indexes");
		OptionBuilder.hasArg();
		OptionBuilder.withArgName("arg");
		options.addOption(OptionBuilder.create('x'));		

		OptionBuilder.withLongOpt("dictFile");
		OptionBuilder.withDescription("Vocabulary file for the category " +
				"(can be estimated as a first step)");
		OptionBuilder.hasArg();
		OptionBuilder.withArgName("FILE");
		options.addOption(OptionBuilder.create('f'));		
		
	}
	
	/* (non-Javadoc)
	 * @see cat.lump.aq.textextraction.wikipedia.cli.WikipediaCliMinimum#parseArguments(java.lang.String[])
	 */
	@Override
	public void parseArguments(String[] args)	{
		super.parseArguments(args);
		
		HelpFormatter formatter = new HelpFormatter();
		int widthFormatter = 88;
		CommandLine cLine = parseLine(args);
		
		if (cLine == null ||
			! ((cLine.hasOption("c") || cLine.hasOption("n")))){				
			logger.error("The category must be defined either with -c or -n\n");
			formatter.printHelp(widthFormatter, command, header, options, footer, true);
			System.exit(1);
		}

		if (cLine.hasOption("s")){
			firstStep = Integer.valueOf(cLine.getOptionValue("s"));
		} else{
			firstStep = 1;
		}
		
		if (cLine.hasOption("e")){
			endStep = Integer.valueOf(cLine.getOptionValue("e"));
		} else{
			endStep = 5;
		}

		if (!cLine.hasOption("d")){				
			if (firstStep < 4){
				logger.error("You must specify the directory with the raw Wikipedia articles\n");
				formatter.printHelp(widthFormatter, command, header, options, footer, true);
				System.exit(1);
			} else{
				inputDir = System.getProperty("user.dir");				
			}				
   	    } else{
		    inputDir = cLine.getOptionValue("d");
	    }

		if (!cLine.hasOption("x")){				
			if (firstStep < 5 && (firstStep>2 && endStep>3)){
				logger.error("You must specify the directory for the indexes\n");
				formatter.printHelp(widthFormatter, command, header, options, footer, true);
				System.exit(1);
			} 
		} else{
   	    	    indexDir = cLine.getOptionValue("x");
	    }

		
		if (!cLine.hasOption("f")){
			if (( firstStep>1 && endStep>3 )){
				logger.error("You must have a Vocabulary file if you skip the first step, " +
						"please, include it with the -f <FILE> option.\n");				
				formatter.printHelp(widthFormatter, command, header, options, footer, true);
				System.exit(1);
			}
	    } else{
		    dictFile = cLine.getOptionValue("f");
	    }
		
		
		if (cLine.hasOption("t")){
			top = Integer.valueOf(cLine.getOptionValue("t"));
		} else{
			top = Integer.valueOf(p.getProperty("topKeywords4L"));
		}

		if (cLine.hasOption("o")){
			outputDir = cLine.getOptionValue("o");			
		} else{
			outputDir = System.getProperty("user.dir");
		} 

		if (cLine.hasOption("c")){
		    sCategory = cLine.getOptionValue("c");          //"Historia_de_Arag√≥n";
			WikipediaJwpl wiki;
			try {
				wiki = new WikipediaJwpl(locale, year);
				iCategory = wiki.getCategory(sCategory).getPageId();
			} catch (WikiApiException e) {
				e.printStackTrace();
			}	
		} else {
			iCategory = Integer.valueOf(cLine.getOptionValue("n"));//50428;
			if (firstStep==1){
				WikipediaJwpl wiki;
				try {
					wiki = new WikipediaJwpl(locale, year);
					sCategory = wiki.getCategory(iCategory).getTitle().getPlainTitle();
					} catch (WikiApiException e) {
						e.printStackTrace();
					}
			}
		}
	}

	
    /** Getters */	
	public String getsCategory()	{
		return sCategory;
	}

	public int getiCategory() {
		return iCategory;
	}
	
	public int getFirstStep()	{
		return firstStep;
	}
	
	public int getLastStep() {
		return endStep;
	}
	
	public String getInputDir()	{
		return inputDir;
	}	

	public String getIndexDir()	{
		return indexDir;
	}	

	public String getOutputDir()	{
		return outputDir;
	}

	public int getTop()	{
		return top;
	}

	public String getDictFile() {
		return dictFile;
	}	
	
}
